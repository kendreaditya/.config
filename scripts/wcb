#!/usr/bin/env python3
"""
wcb - Web Context Builder

Async web scraper that converts websites to LLM-optimized markdown.

Usage: wcb https://docs.example.com
       wcb https://docs.example.com -o ./my-docs -d 3
       wcb docs.example.com --cross-subdomain
"""
import os
import sys
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from _utils import ensure_config_venv
ensure_config_venv()

import asyncio
import hashlib
import re
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from typing import Optional, Set
from urllib.parse import urljoin, urlparse

import aiofiles
import aiohttp
import click
import tldextract
from bs4 import BeautifulSoup, Comment, Tag
from markdownify import MarkdownConverter
from rich.console import Console, Group
from rich.live import Live
from rich.panel import Panel
from rich.progress import (
    BarColumn,
    Progress,
    SpinnerColumn,
    TaskID,
    TextColumn,
    TimeElapsedColumn,
)
from rich.table import Table
from rich.text import Text
from rich.tree import Tree

# Playwright is an optional dependency
try:
    from playwright.async_api import Browser, Page, Playwright, async_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False

__version__ = "1.0.0"


# =============================================================================
# Configuration
# =============================================================================

def url_to_clean_filename(url: str) -> str:
    """Convert a URL to a clean filename."""
    parsed = urlparse(url)
    hostname = parsed.netloc or parsed.path.split("/")[0]
    hostname = hostname.split(":")[0]
    hostname = re.sub(r"[^\w\-.]", "-", hostname)
    return f"{hostname}.md"


@dataclass
class CrawlerConfig:
    """Configuration for the web crawler."""

    root_url: str
    max_concurrent: int = 5
    max_depth: Optional[int] = None
    delay_between_requests: float = 0.1
    request_timeout: int = 30
    max_retries: int = 3
    stay_on_subdomain: bool = True
    url_include_patterns: list[str] = field(default_factory=list)
    url_exclude_patterns: list[str] = field(default_factory=list)
    output_dir: Path = field(default_factory=lambda: Path("./output"))
    merged_filename: Optional[str] = None
    exclude_extensions: list[str] = field(default_factory=lambda: [
        r".*\.(pdf|zip|tar|gz|exe|dmg|pkg|deb|rpm)$",
        r".*\.(png|jpg|jpeg|gif|svg|ico|webp)$",
        r".*\.(css|js|woff|woff2|ttf|eot)$",
        r".*\.(mp3|mp4|wav|avi|mov|webm)$",
    ])
    user_agent: str = "WebContextBuilder/1.0 (LLM Context Scraper)"
    use_browser: bool = False
    browser_headless: bool = True

    def __post_init__(self) -> None:
        if isinstance(self.output_dir, str):
            self.output_dir = Path(self.output_dir)
        if not self.root_url.startswith(("http://", "https://")):
            self.root_url = f"https://{self.root_url}"
        self.root_url = self.root_url.rstrip("/")
        if self.merged_filename is None:
            self.merged_filename = url_to_clean_filename(self.root_url)


# =============================================================================
# HTML Parser
# =============================================================================

class LLMOptimizedConverter(MarkdownConverter):
    """Custom markdown converter optimized for LLM consumption."""

    def __init__(self, base_url: str, **kwargs):
        self.base_url = base_url
        super().__init__(**kwargs)

    def convert_a(self, el: Tag, text: str, convert_as_inline: bool = False, **kwargs) -> str:
        href = el.get("href", "")
        if href and not href.startswith(("http://", "https://", "mailto:", "#")):
            href = urljoin(self.base_url, href)
        title = el.get("title", "")
        text = text.strip()
        if not text or not href:
            return text or ""
        if title:
            return f'[{text}]({href} "{title}")'
        return f"[{text}]({href})"

    def convert_img(self, el: Tag, text: str, convert_as_inline: bool = False, **kwargs) -> str:
        src = el.get("src", "")
        if src and not src.startswith(("http://", "https://", "data:")):
            src = urljoin(self.base_url, src)
        alt = el.get("alt", "")
        title = el.get("title", "")
        if not src:
            return ""
        if title:
            return f'![{alt}]({src} "{title}")'
        return f"![{alt}]({src})"


REMOVE_SELECTORS = [
    "nav", "header", "footer", "aside",
    ".sidebar", ".navigation", ".nav", ".menu",
    ".header", ".footer", ".breadcrumb", ".breadcrumbs",
    ".toc", ".table-of-contents",
    ".advertisement", ".ads", ".ad",
    ".social-share", ".social-links", ".share-buttons",
    ".cookie-banner", ".cookie-notice", ".popup", ".modal",
    "#sidebar", "#nav", "#navigation", "#header", "#footer",
    "[role='navigation']", "[role='banner']", "[role='contentinfo']",
    "[aria-label='breadcrumb']",
    "script", "style", "noscript", "iframe",
    "form", "button", "input", "select", "textarea",
]


def clean_html(soup: BeautifulSoup) -> BeautifulSoup:
    """Remove non-content elements from HTML."""
    for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):
        comment.extract()
    for selector in REMOVE_SELECTORS:
        for element in soup.select(selector):
            element.decompose()
    for element in soup.find_all():
        if isinstance(element, Tag):
            if element.name not in ["br", "hr", "img"]:
                if not element.get_text(strip=True) and not element.find("img"):
                    element.decompose()
    return soup


def extract_title(soup: BeautifulSoup) -> str:
    """Extract the page title."""
    h1 = soup.find("h1")
    if h1:
        return h1.get_text(strip=True)
    title = soup.find("title")
    if title:
        return title.get_text(strip=True)
    return "Untitled"


def extract_main_content(soup: BeautifulSoup) -> Optional[Tag]:
    """Extract the main content area of the page."""
    selectors = [
        "main", "article", "[role='main']",
        ".main-content", ".content", ".post-content",
        ".article-content", ".entry-content",
        "#main", "#content", "#main-content",
        ".markdown-body", ".documentation", ".docs-content",
    ]
    for selector in selectors:
        content = soup.select_one(selector)
        if content:
            return content
    return soup.body


def html_to_markdown(html: str, url: str) -> str:
    """Convert HTML to LLM-optimized markdown."""
    soup = BeautifulSoup(html, "lxml")
    title = extract_title(soup)
    soup = clean_html(soup)
    main_content = extract_main_content(soup)

    if not main_content:
        return f"# {title}\n\n*No content extracted*"

    converter = LLMOptimizedConverter(
        base_url=url,
        heading_style="atx",
        bullets="-",
        code_language_callback=lambda el: el.get("class", [""])[0].replace("language-", "") if el.get("class") else "",
    )
    markdown = converter.convert(str(main_content))
    markdown = clean_markdown(markdown)

    if not markdown.strip().startswith("#"):
        markdown = f"# {title}\n\n{markdown}"
    return markdown


def clean_markdown(markdown: str) -> str:
    """Clean up the generated markdown."""
    markdown = re.sub(r"\n{3,}", "\n\n", markdown)
    lines = [line.rstrip() for line in markdown.split("\n")]
    markdown = "\n".join(lines)
    markdown = re.sub(r"\[([^\]]*)\]\(\s*\)", r"\1", markdown)
    markdown = re.sub(r"^\s*-\s*$", "", markdown, flags=re.MULTILINE)
    markdown = re.sub(r"  +", " ", markdown)
    markdown = re.sub(r"(\n#)", r"\n\1", markdown)
    return markdown.strip()


# =============================================================================
# Storage
# =============================================================================

def url_to_filename(url: str) -> str:
    """Convert a URL to a safe filename."""
    parsed = urlparse(url)
    path = parsed.path.strip("/") or "index"
    filename = path.replace("/", "_")
    filename = re.sub(r"[^\w\-_.]", "_", filename)
    url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
    if len(filename) > 200:
        filename = filename[:200]
    return f"{filename}_{url_hash}.md"


class StorageManager:
    """Manages storage of scraped content."""

    def __init__(self, output_dir: Path, merged_filename: str = "merged_content.md"):
        self.output_dir = output_dir
        self.pages_dir = output_dir / "pages"
        self.merged_path = output_dir / merged_filename
        self._lock = asyncio.Lock()
        self._saved_files: list[tuple[str, Path]] = []

    async def initialize(self) -> None:
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.pages_dir.mkdir(parents=True, exist_ok=True)

    async def save_page(self, url: str, markdown: str) -> Path:
        filename = url_to_filename(url)
        filepath = self.pages_dir / filename
        content = f"<!-- Source: {url} -->\n\n{markdown}"
        async with aiofiles.open(filepath, "w", encoding="utf-8") as f:
            await f.write(content)
        async with self._lock:
            self._saved_files.append((url, filepath))
        return filepath

    async def merge_all(self, separator: str = "\n\n---\n\n") -> Path:
        async with self._lock:
            files = list(self._saved_files)
        files.sort(key=lambda x: x[0])

        merged_content = []
        merged_content.append("# Merged Documentation\n")
        merged_content.append(f"**Total Pages:** {len(files)}\n")
        merged_content.append("\n## Table of Contents\n")

        for i, (url, _) in enumerate(files, 1):
            safe_anchor = re.sub(r"[^\w\-]", "-", url)
            merged_content.append(f"{i}. [{url}](#{safe_anchor})\n")
        merged_content.append(separator)

        for url, filepath in files:
            async with aiofiles.open(filepath, "r", encoding="utf-8") as f:
                content = await f.read()
            safe_anchor = re.sub(r"[^\w\-]", "-", url)
            merged_content.append(f'<a id="{safe_anchor}"></a>\n\n')
            merged_content.append(f"## Source: {url}\n\n")
            merged_content.append(content)
            merged_content.append(separator)

        async with aiofiles.open(self.merged_path, "w", encoding="utf-8") as f:
            await f.write("".join(merged_content))
        return self.merged_path

    @property
    def saved_count(self) -> int:
        return len(self._saved_files)

    def get_saved_files(self) -> list[tuple[str, Path]]:
        return list(self._saved_files)


# =============================================================================
# Visualizer
# =============================================================================

class PageStatus(Enum):
    PENDING = "pending"
    CRAWLING = "crawling"
    SUCCESS = "success"
    FAILED = "failed"
    SKIPPED = "skipped"


@dataclass
class PageInfo:
    url: str
    depth: int
    status: PageStatus = PageStatus.PENDING
    title: Optional[str] = None
    links_found: int = 0
    error: Optional[str] = None
    parent_url: Optional[str] = None


@dataclass
class CrawlStats:
    pages_discovered: int = 0
    pages_crawled: int = 0
    pages_failed: int = 0
    pages_skipped: int = 0
    total_links_found: int = 0
    current_depth: int = 0
    max_depth_reached: int = 0

    @property
    def pages_remaining(self) -> int:
        return self.pages_discovered - self.pages_crawled - self.pages_failed - self.pages_skipped


class CrawlerVisualizer:
    """Rich CLI visualization for web crawling progress."""

    def __init__(self, root_url: str, max_depth: Optional[int] = None):
        self.console = Console()
        self.root_url = root_url
        self.max_depth = max_depth
        self.stats = CrawlStats()
        self.pages: dict[str, PageInfo] = {}
        self._lock = asyncio.Lock()
        self._live: Optional[Live] = None

        self._progress = Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TextColumn("({task.completed}/{task.total})"),
            TimeElapsedColumn(),
        )
        self._main_task: Optional[TaskID] = None
        self._url_tree: dict[str, list[str]] = {}

    async def start(self) -> None:
        self._main_task = self._progress.add_task("[cyan]Crawling pages...", total=None)
        self._live = Live(
            self._generate_display(),
            console=self.console,
            refresh_per_second=4,
            transient=False,
        )
        self._live.start()

    async def stop(self) -> None:
        if self._live:
            self._live.stop()

    async def add_page(self, url: str, depth: int, parent_url: Optional[str] = None) -> None:
        async with self._lock:
            if url not in self.pages:
                self.pages[url] = PageInfo(url=url, depth=depth, parent_url=parent_url)
                self.stats.pages_discovered += 1
                self.stats.max_depth_reached = max(self.stats.max_depth_reached, depth)
                if parent_url:
                    if parent_url not in self._url_tree:
                        self._url_tree[parent_url] = []
                    self._url_tree[parent_url].append(url)
                else:
                    self._url_tree[url] = []
                self._update_progress()
                self._refresh_display()

    async def update_page(
        self, url: str, status: PageStatus,
        title: Optional[str] = None, links_found: int = 0, error: Optional[str] = None,
    ) -> None:
        async with self._lock:
            if url in self.pages:
                page = self.pages[url]
                page.status = status
                page.title = title
                page.links_found = links_found
                page.error = error
                if status == PageStatus.SUCCESS:
                    self.stats.pages_crawled += 1
                    self.stats.total_links_found += links_found
                elif status == PageStatus.FAILED:
                    self.stats.pages_failed += 1
                elif status == PageStatus.SKIPPED:
                    self.stats.pages_skipped += 1
                self._update_progress()
                self._refresh_display()

    def _update_progress(self) -> None:
        if self._main_task is not None:
            completed = self.stats.pages_crawled + self.stats.pages_failed + self.stats.pages_skipped
            self._progress.update(self._main_task, completed=completed, total=self.stats.pages_discovered)

    def _refresh_display(self) -> None:
        if self._live:
            self._live.update(self._generate_display())

    def _generate_display(self) -> Panel:
        stats_table = Table(show_header=False, box=None, padding=(0, 2))
        stats_table.add_column("Metric", style="cyan")
        stats_table.add_column("Value", style="green")
        stats_table.add_row("Discovered", str(self.stats.pages_discovered))
        stats_table.add_row("Crawled", str(self.stats.pages_crawled))
        stats_table.add_row("Failed", str(self.stats.pages_failed))
        stats_table.add_row("Skipped", str(self.stats.pages_skipped))
        stats_table.add_row("Remaining", str(self.stats.pages_remaining))
        stats_table.add_row("Max Depth", str(self.stats.max_depth_reached))
        stats_table.add_row("Links Found", str(self.stats.total_links_found))

        tree = self._build_url_tree()
        recent = self._get_recent_activity()

        content = Group(
            self._progress, Text(), stats_table, Text(),
            Panel(tree, title="[bold]Crawl Tree[/bold]", border_style="blue"), Text(),
            Panel(recent, title="[bold]Recent Activity[/bold]", border_style="green"),
        )

        depth_str = f"/{self.max_depth}" if self.max_depth else ""
        return Panel(
            content,
            title=f"[bold cyan]Web Context Builder[/bold cyan] - Crawling {self._truncate_url(self.root_url)}",
            subtitle=f"Depth: {self.stats.max_depth_reached}{depth_str}",
            border_style="cyan",
        )

    def _build_url_tree(self, max_items: int = 15) -> Tree:
        tree = Tree(f"[bold]{self._truncate_url(self.root_url)}[/bold]")

        def add_children(parent_tree: Tree, parent_url: str, depth: int = 0, count: int = 0) -> int:
            if depth > 3 or count >= max_items:
                return count
            children = self._url_tree.get(parent_url, [])
            for child_url in children[:5]:
                if count >= max_items:
                    break
                page = self.pages.get(child_url)
                if page:
                    status_icon = self._get_status_icon(page.status)
                    label = f"{status_icon} {self._truncate_url(child_url, 50)}"
                    if page.title and page.status == PageStatus.SUCCESS:
                        label = f"{status_icon} {page.title[:40]}"
                    child_tree = parent_tree.add(label)
                    count = add_children(child_tree, child_url, depth + 1, count + 1)
            if len(children) > 5:
                parent_tree.add(f"[dim]... and {len(children) - 5} more[/dim]")
            return count

        add_children(tree, self.root_url)
        return tree

    def _get_recent_activity(self, limit: int = 5) -> Table:
        table = Table(show_header=True, box=None, padding=(0, 1))
        table.add_column("Status", width=8)
        table.add_column("URL", overflow="ellipsis")
        table.add_column("Links", justify="right", width=6)

        recent_pages = sorted(
            self.pages.values(),
            key=lambda p: (p.status != PageStatus.CRAWLING, p.depth),
        )[:limit]

        for page in recent_pages:
            status_text = self._get_status_text(page.status)
            links = str(page.links_found) if page.links_found else "-"
            table.add_row(status_text, self._truncate_url(page.url, 60), links)
        return table

    def _get_status_icon(self, status: PageStatus) -> str:
        icons = {
            PageStatus.PENDING: "[yellow]○[/yellow]",
            PageStatus.CRAWLING: "[cyan]◉[/cyan]",
            PageStatus.SUCCESS: "[green]✓[/green]",
            PageStatus.FAILED: "[red]✗[/red]",
            PageStatus.SKIPPED: "[dim]○[/dim]",
        }
        return icons.get(status, "○")

    def _get_status_text(self, status: PageStatus) -> str:
        texts = {
            PageStatus.PENDING: "[yellow]Pending[/yellow]",
            PageStatus.CRAWLING: "[cyan]Crawling[/cyan]",
            PageStatus.SUCCESS: "[green]Done[/green]",
            PageStatus.FAILED: "[red]Failed[/red]",
            PageStatus.SKIPPED: "[dim]Skipped[/dim]",
        }
        return texts.get(status, "Unknown")

    def _truncate_url(self, url: str, max_len: int = 40) -> str:
        parsed = urlparse(url)
        path = parsed.path or "/"
        if len(path) > max_len:
            return f"...{path[-(max_len-3):]}"
        return path

    async def print_summary(self) -> None:
        self.console.print()
        self.console.print("[bold green]✓ Crawl Complete![/bold green]")
        self.console.print()

        table = Table(title="Crawl Summary", show_header=True)
        table.add_column("Metric", style="cyan")
        table.add_column("Value", style="green", justify="right")
        table.add_row("Pages Discovered", str(self.stats.pages_discovered))
        table.add_row("Pages Crawled", str(self.stats.pages_crawled))
        table.add_row("Pages Failed", str(self.stats.pages_failed))
        table.add_row("Pages Skipped", str(self.stats.pages_skipped))
        table.add_row("Total Links Found", str(self.stats.total_links_found))
        table.add_row("Max Depth Reached", str(self.stats.max_depth_reached))
        self.console.print(table)
        self.console.print()


# =============================================================================
# Browser Fetcher (optional Playwright)
# =============================================================================

def check_playwright_available() -> bool:
    return PLAYWRIGHT_AVAILABLE


class BrowserFetcher:
    """Fetches pages using a headless browser for JS-rendered content."""

    def __init__(self, headless: bool = True, timeout: int = 30000, wait_for_idle: bool = True):
        if not PLAYWRIGHT_AVAILABLE:
            raise ImportError(
                "Playwright is not installed. Install it with:\n"
                "  pip install playwright\n"
                "  playwright install chromium"
            )
        self.headless = headless
        self.timeout = timeout
        self.wait_for_idle = wait_for_idle
        self._playwright: Optional[Playwright] = None
        self._browser: Optional[Browser] = None

    async def start(self) -> None:
        self._playwright = await async_playwright().start()
        self._browser = await self._playwright.chromium.launch(headless=self.headless)

    async def stop(self) -> None:
        if self._browser:
            await self._browser.close()
        if self._playwright:
            await self._playwright.stop()

    async def fetch(self, url: str) -> Optional[str]:
        if not self._browser:
            raise RuntimeError("Browser not started. Call start() first.")
        page: Optional[Page] = None
        try:
            page = await self._browser.new_page()
            response = await page.goto(url, timeout=self.timeout, wait_until="domcontentloaded")
            if not response:
                return None
            content_type = response.headers.get("content-type", "")
            if "text/html" not in content_type.lower():
                return None
            if self.wait_for_idle:
                try:
                    await page.wait_for_load_state("networkidle", timeout=10000)
                except Exception:
                    pass
            return await page.content()
        except Exception:
            return None
        finally:
            if page:
                await page.close()

    async def __aenter__(self) -> "BrowserFetcher":
        await self.start()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:
        await self.stop()


# =============================================================================
# Core Crawler
# =============================================================================

class WebCrawler:
    """Production-ready async web crawler."""

    def __init__(self, config: CrawlerConfig):
        self.config = config
        self.storage = StorageManager(config.output_dir, config.merged_filename)
        self.visualizer: Optional[CrawlerVisualizer] = None
        self._browser: Optional[BrowserFetcher] = None

        self._seen_urls: Set[str] = set()
        self._url_lock = asyncio.Lock()

        self._root_extract = tldextract.extract(config.root_url)
        self._root_parsed = urlparse(config.root_url)

        self._exclude_extensions = [
            re.compile(pattern, re.IGNORECASE)
            for pattern in config.exclude_extensions
        ]
        self._url_include_patterns = [
            re.compile(pattern, re.IGNORECASE)
            for pattern in config.url_include_patterns
        ] if config.url_include_patterns else []
        self._url_exclude_patterns = [
            re.compile(pattern, re.IGNORECASE)
            for pattern in config.url_exclude_patterns
        ]

        self._semaphore = asyncio.Semaphore(config.max_concurrent)
        self._queue: asyncio.Queue[tuple[str, int, Optional[str]]] = asyncio.Queue()
        self._active_tasks = 0
        self._active_lock = asyncio.Lock()
        self._finished = asyncio.Event()

    def _normalize_url(self, url: str) -> str:
        parsed = urlparse(url)
        normalized = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        if normalized.endswith("/") and parsed.path != "/":
            normalized = normalized[:-1]
        if parsed.query:
            params = sorted(parsed.query.split("&"))
            normalized = f"{normalized}?{'&'.join(params)}"
        return normalized.lower()

    def _is_same_domain(self, url: str) -> bool:
        try:
            extract = tldextract.extract(url)
            if extract.top_domain_under_public_suffix != self._root_extract.top_domain_under_public_suffix:
                return False
            if self.config.stay_on_subdomain:
                if extract.subdomain != self._root_extract.subdomain:
                    return False
            return True
        except Exception:
            return False

    def _should_crawl(self, url: str) -> bool:
        if not url.startswith(("http://", "https://")):
            return False
        if not self._url_include_patterns and not self._is_same_domain(url):
            return False
        for pattern in self._exclude_extensions:
            if pattern.search(url):
                return False
        for pattern in self._url_exclude_patterns:
            if pattern.search(url):
                return False
        if self._url_include_patterns:
            if not any(pattern.search(url) for pattern in self._url_include_patterns):
                return False
        return True

    def _extract_links(self, html: str, base_url: str) -> list[str]:
        soup = BeautifulSoup(html, "lxml")
        links = []
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if not href or href.startswith(("javascript:", "mailto:", "tel:", "#")):
                continue
            absolute_url = urljoin(base_url, href)
            normalized = self._normalize_url(absolute_url)
            if self._should_crawl(normalized):
                links.append(normalized)
        return list(set(links))

    async def _fetch_page(self, session: aiohttp.ClientSession, url: str) -> Optional[str]:
        for attempt in range(self.config.max_retries):
            try:
                async with session.get(
                    url,
                    timeout=aiohttp.ClientTimeout(total=self.config.request_timeout),
                    allow_redirects=True,
                    headers={"User-Agent": self.config.user_agent},
                ) as response:
                    content_type = response.headers.get("Content-Type", "")
                    if "text/html" not in content_type.lower():
                        return None
                    if response.status == 200:
                        return await response.text()
                    elif response.status in (404, 410):
                        return None
                    else:
                        if response.status >= 500:
                            await asyncio.sleep(2**attempt)
                            continue
                        return None
            except asyncio.TimeoutError:
                if attempt < self.config.max_retries - 1:
                    await asyncio.sleep(2**attempt)
                continue
            except aiohttp.ClientError:
                if attempt < self.config.max_retries - 1:
                    await asyncio.sleep(2**attempt)
                continue
            except Exception:
                return None
        return None

    async def _fetch_page_browser(self, url: str) -> Optional[str]:
        if not self._browser:
            return None
        for attempt in range(self.config.max_retries):
            try:
                html = await self._browser.fetch(url)
                if html:
                    return html
            except Exception:
                if attempt < self.config.max_retries - 1:
                    await asyncio.sleep(2**attempt)
                continue
        return None

    async def _process_page(
        self, session: Optional[aiohttp.ClientSession],
        url: str, depth: int, parent_url: Optional[str],
    ) -> None:
        async with self._semaphore:
            if self.visualizer:
                await self.visualizer.update_page(url, PageStatus.CRAWLING)
            try:
                if self.config.use_browser:
                    html = await self._fetch_page_browser(url)
                else:
                    html = await self._fetch_page(session, url)

                if html is None:
                    if self.visualizer:
                        await self.visualizer.update_page(url, PageStatus.SKIPPED, error="Not HTML or fetch failed")
                    return

                links = self._extract_links(html, url)
                markdown = html_to_markdown(html, url)

                title = None
                for line in markdown.split("\n"):
                    if line.startswith("# "):
                        title = line[2:].strip()
                        break

                await self.storage.save_page(url, markdown)

                if self.visualizer:
                    await self.visualizer.update_page(url, PageStatus.SUCCESS, title=title, links_found=len(links))

                if self.config.max_depth is None or depth < self.config.max_depth:
                    for link in links:
                        await self._maybe_queue_url(link, depth + 1, url)

                if self.config.delay_between_requests > 0:
                    await asyncio.sleep(self.config.delay_between_requests)

            except Exception as e:
                if self.visualizer:
                    await self.visualizer.update_page(url, PageStatus.FAILED, error=str(e))

    async def _maybe_queue_url(self, url: str, depth: int, parent_url: Optional[str]) -> bool:
        async with self._url_lock:
            if url in self._seen_urls:
                return False
            self._seen_urls.add(url)
        if self.visualizer:
            await self.visualizer.add_page(url, depth, parent_url)
        await self._queue.put((url, depth, parent_url))
        return True

    async def _worker(self, session: Optional[aiohttp.ClientSession]) -> None:
        while True:
            try:
                try:
                    url, depth, parent_url = await asyncio.wait_for(self._queue.get(), timeout=2.0)
                except asyncio.TimeoutError:
                    async with self._active_lock:
                        if self._active_tasks == 0 and self._queue.empty():
                            return
                    continue

                async with self._active_lock:
                    self._active_tasks += 1
                try:
                    await self._process_page(session, url, depth, parent_url)
                finally:
                    self._queue.task_done()
                    async with self._active_lock:
                        self._active_tasks -= 1

            except asyncio.CancelledError:
                return

    async def crawl(self, show_progress: bool = True) -> int:
        await self.storage.initialize()

        if show_progress:
            self.visualizer = CrawlerVisualizer(self.config.root_url, self.config.max_depth)
            await self.visualizer.start()

        normalized_root = self._normalize_url(self.config.root_url)
        await self._maybe_queue_url(normalized_root, 0, None)

        if self.config.use_browser:
            self._browser = BrowserFetcher(
                headless=self.config.browser_headless,
                timeout=self.config.request_timeout * 1000,
            )
            await self._browser.start()

        try:
            if self.config.use_browser:
                workers = [asyncio.create_task(self._worker(None)) for _ in range(self.config.max_concurrent)]
                await self._queue.join()
                for worker in workers:
                    worker.cancel()
                await asyncio.gather(*workers, return_exceptions=True)
            else:
                connector = aiohttp.TCPConnector(limit=self.config.max_concurrent * 2)
                async with aiohttp.ClientSession(connector=connector) as session:
                    workers = [asyncio.create_task(self._worker(session)) for _ in range(self.config.max_concurrent)]
                    await self._queue.join()
                    for worker in workers:
                        worker.cancel()
                    await asyncio.gather(*workers, return_exceptions=True)
        finally:
            if self._browser:
                await self._browser.stop()
            if self.visualizer:
                await self.visualizer.stop()
                await self.visualizer.print_summary()

        return self.storage.saved_count

    async def merge_results(self) -> str:
        merged_path = await self.storage.merge_all()
        return str(merged_path)


# =============================================================================
# CLI
# =============================================================================

console = Console()


def print_banner() -> None:
    banner = """
[bold cyan]╔══════════════════════════════════════════════════════════════╗
║                    Web Context Builder                        ║
║              Production-Ready Web Scraper for LLMs            ║
╚══════════════════════════════════════════════════════════════╝[/bold cyan]
"""
    console.print(banner)


@click.command()
@click.argument("url")
@click.option("-o", "--output", default="./output", type=click.Path(), help="Output directory for scraped content")
@click.option("-c", "--concurrent", default=5, type=int, help="Maximum concurrent requests (default: 5)")
@click.option("-d", "--depth", default=None, type=int, help="Maximum crawl depth (default: unlimited)")
@click.option("--delay", default=0.1, type=float, help="Delay between requests in seconds (default: 0.1)")
@click.option("--timeout", default=30, type=int, help="Request timeout in seconds (default: 30)")
@click.option("--cross-subdomain/--same-subdomain", default=False, help="Allow crawling across subdomains")
@click.option("--no-progress", is_flag=True, default=False, help="Disable live progress visualization")
@click.option("--no-merge", is_flag=True, default=False, help="Skip merging into single file")
@click.option("-m", "--merged-name", default=None, help="Name of the merged output file (default: <domain>.md)")
@click.option("--browser", is_flag=True, default=False, help="Use headless browser for JS-rendered content")
@click.option("--browser-visible", is_flag=True, default=False, help="Show browser window when using --browser")
@click.option("-i", "--include", "include_patterns", multiple=True, help="Regex pattern for URLs to include")
@click.option("-e", "--exclude", "exclude_patterns", multiple=True, help="Regex pattern for URLs to exclude")
@click.version_option(version=__version__)
def main(
    url: str, output: str, concurrent: int, depth: int | None,
    delay: float, timeout: int, cross_subdomain: bool,
    no_progress: bool, no_merge: bool, merged_name: str | None,
    browser: bool, browser_visible: bool,
    include_patterns: tuple[str, ...], exclude_patterns: tuple[str, ...],
) -> None:
    """Web Context Builder - Scrape websites to LLM-optimized markdown.

    URL is the starting page to crawl. Only pages on the same domain/subdomain
    will be scraped.

    Examples:

        wcb https://docs.example.com

        wcb https://docs.example.com -o ./my-docs -d 3

        wcb https://docs.example.com --cross-subdomain

        wcb https://js-heavy-site.com --browser

        wcb https://docs.example.com -i '/api/' -i '/guide/'
    """
    print_banner()

    if browser and not check_playwright_available():
        console.print("[red]Error: Playwright is not installed.[/red]")
        console.print("[yellow]Install it with:[/yellow]")
        console.print("  pip install playwright")
        console.print("  playwright install chromium")
        sys.exit(1)

    if not url.startswith(("http://", "https://")):
        url = f"https://{url}"

    config = CrawlerConfig(
        root_url=url,
        output_dir=Path(output),
        max_concurrent=concurrent,
        max_depth=depth,
        delay_between_requests=delay,
        request_timeout=timeout,
        stay_on_subdomain=not cross_subdomain,
        merged_filename=merged_name,
        use_browser=browser,
        browser_headless=not browser_visible,
        url_include_patterns=list(include_patterns),
        url_exclude_patterns=list(exclude_patterns),
    )

    console.print(f"[bold]Starting crawl:[/bold] {url}")
    console.print(f"[dim]Output directory: {config.output_dir.absolute()}[/dim]")
    console.print(f"[dim]Max concurrent: {concurrent} | Max depth: {depth or 'unlimited'}[/dim]")
    console.print(f"[dim]Subdomain restriction: {'Same subdomain only' if not cross_subdomain else 'Cross-subdomain allowed'}[/dim]")
    if include_patterns:
        console.print(f"[dim]Include patterns: {', '.join(include_patterns)}[/dim]")
    if exclude_patterns:
        console.print(f"[dim]Exclude patterns: {', '.join(exclude_patterns)}[/dim]")
    if browser:
        console.print(f"[dim]Browser mode: {'visible' if browser_visible else 'headless'}[/dim]")
    console.print()

    try:
        crawler = WebCrawler(config)
        pages_crawled = asyncio.run(crawler.crawl(show_progress=not no_progress))

        if pages_crawled == 0:
            console.print("[yellow]No pages were successfully crawled.[/yellow]")
            sys.exit(1)

        console.print(f"[green]Successfully crawled {pages_crawled} pages[/green]")

        if not no_merge:
            console.print("[dim]Merging pages into single file...[/dim]")
            merged_path = asyncio.run(crawler.merge_results())
            console.print(f"[green]Merged file created: {merged_path}[/green]")

        console.print()
        console.print("[bold]Output files:[/bold]")
        console.print(f"  Individual pages: {config.output_dir / 'pages'}")
        if not no_merge:
            console.print(f"  Merged file: {config.output_dir / config.merged_filename}")

    except KeyboardInterrupt:
        console.print("\n[yellow]Crawl interrupted by user[/yellow]")
        sys.exit(130)
    except Exception as e:
        console.print(f"[red]Error: {e}[/red]")
        sys.exit(1)


if __name__ == "__main__":
    main()
