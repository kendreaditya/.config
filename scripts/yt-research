#!/usr/bin/env python3
"""
yt-research - Stream YouTube channel transcripts to Markdown

Usage: yt-research @channel_name
       yt-research https://www.youtube.com/@channel_name
       yt-research "search query terms"
       yt-research https://www.youtube.com/results?search_query=...
       yt-research -n 20 "search query terms"

Output: ./[channel_name].md or ./[search_query].md in current directory
"""
import os
import sys
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from _utils import ensure_config_venv
ensure_config_venv()

import asyncio
import argparse
import random
import re
import time
import urllib.error
import urllib.parse
import urllib.request
from pathlib import Path
from typing import Optional
import yt_dlp
from _utils import sanitize_filename, ProgressLogger, MarkdownWriter
from _context import split_sentences


def get_channel_name(channel_input: str) -> str:
    """Extract channel name from input (handle @name or URL)."""
    if channel_input.startswith('@'):
        return channel_input[1:]

    match = re.search(r'@([^/\s]+)', channel_input)
    if match:
        return match.group(1)

    match = re.search(r'/(?:c|channel)/([^/\s]+)', channel_input)
    if match:
        return match.group(1)

    return channel_input.strip('/')


def get_channel_url(channel_input: str) -> str:
    """Convert input to full channel URL."""
    if channel_input.startswith('http'):
        return channel_input
    if channel_input.startswith('@'):
        return f'https://www.youtube.com/{channel_input}'
    return f'https://www.youtube.com/@{channel_input}'


def is_search_input(input_str: str) -> bool:
    """Check if the input is a YouTube search URL or a plain search query."""
    if 'youtube.com/results' in input_str and 'search_query=' in input_str:
        return True
    # Not a channel URL or @handle → treat as search query
    if not input_str.startswith('@') and not input_str.startswith('http'):
        return True
    return False


def get_search_query(input_str: str) -> str:
    """Extract search query from a YouTube search URL or return raw query."""
    if 'youtube.com/results' in input_str and 'search_query=' in input_str:
        parsed = urllib.parse.urlparse(input_str)
        params = urllib.parse.parse_qs(parsed.query)
        return params.get('search_query', [input_str])[0]
    return input_str


def clean_vtt_content(vtt_text: str) -> str:
    """Clean VTT format, return plain text with substring deduplication."""
    lines = []

    for line in vtt_text.split('\n'):
        if line.startswith('WEBVTT') or '-->' in line:
            continue
        if line.startswith(('Kind:', 'Language:')):
            continue
        if not line.strip():
            continue

        line = re.sub(r'<[\d:.]+>', '', line)
        line = re.sub(r'</?c[^>]*>', '', line)
        line = re.sub(r'</?[a-z][^>]*>', '', line)
        line = line.strip()

        if not line:
            continue

        # Hybrid deduplication: 'in' checks first, then overlap with min length
        MIN_OVERLAP = 4  # Minimum overlap to avoid false positives
        if lines:
            last = lines[-1]

            # First: try substring checks (catches most cases)
            if line in last:
                continue
            if last in line:
                lines[-1] = line
                continue

            # Second: try overlap merge with minimum length
            min_len = min(len(last), len(line))
            for overlap_len in range(min_len, MIN_OVERLAP - 1, -1):
                if last[-overlap_len:] == line[:overlap_len]:
                    lines[-1] = last + line[overlap_len:]
                    break
            else:
                lines.append(line)
            continue

        lines.append(line)

    text = ' '.join(lines)
    text = re.sub(r'\s+', ' ', text)
    text = '\n\n'.join(split_sentences(text))

    return text.strip()


def format_markdown_section(video_id: str, title: str, text: str) -> str:
    """Format a video transcript as a Markdown section."""
    return f"""## {title}
**Video ID:** {video_id}
**URL:** https://youtube.com/watch?v={video_id}

{text}

---

"""


def get_existing_video_ids(output_path: Path) -> set[str]:
    """Parse existing markdown file and extract already-processed video IDs."""
    if not output_path.exists():
        return set()

    try:
        content = output_path.read_text(encoding='utf-8')
        video_ids = re.findall(r'\*\*Video ID:\*\* ([a-zA-Z0-9_-]+)', content)
        return set(video_ids)
    except Exception:
        return set()


class RateLimitMonitor:
    """Track consecutive rate-limit failures and prompt user when threshold hit."""

    def __init__(self, threshold: int = 3):
        self.threshold = threshold
        self._consecutive_failures = 0
        self._abort = False
        self._lock = asyncio.Lock()
        self._gate = asyncio.Event()
        self._gate.set()  # Start open

    @property
    def should_abort(self) -> bool:
        return self._abort

    async def record_success(self):
        async with self._lock:
            self._consecutive_failures = 0

    async def record_rate_limit(self):
        async with self._lock:
            self._consecutive_failures += 1
            if self._consecutive_failures >= self.threshold and not self._abort:
                self._gate.clear()
                await self._prompt_user()

    async def _prompt_user(self):
        print(f"\n⚠  {self._consecutive_failures} consecutive videos rate-limited.")
        print("Options:")
        print("  [w] Wait 60s and retry (default)")
        print("  [a] Abort remaining videos")

        loop = asyncio.get_event_loop()
        answer = await loop.run_in_executor(None, lambda: input("Choice [w/a]: ").strip().lower())

        if answer == 'a':
            self._abort = True
            self._gate.set()  # Unblock workers so they see abort flag
            print("Aborting remaining videos.")
        else:
            wait_time = 60
            print(f"Waiting {wait_time}s before resuming...")
            await asyncio.sleep(wait_time)
            self._consecutive_failures = 0
            self._gate.set()
            print("Resuming.")

    async def wait_if_paused(self):
        """Block until rate-limit pause is resolved."""
        await self._gate.wait()


YT_PROGRESS_METRICS = [
    {"name": "processed", "color": "green"},
    {"name": "resumed", "color": "blue"},
    {"name": "skipped", "color": "yellow"},
    {"name": "rate_limited", "color": "red"},
    {"name": "failed", "color": "red"},
]


def _try_fetch_subtitles(video_url: str, ydl_opts: dict) -> tuple[Optional[str], Optional[str]]:
    """Single attempt to fetch subtitles. Returns (content, error_reason)."""
    opts = {
        **ydl_opts,
        'writeautomaticsub': True,
        'writesubtitles': True,
        'subtitlesformat': 'vtt',
        'subtitleslangs': ['en', 'en-US', 'en-GB', 'en-orig'],
        'skip_download': True,
    }

    with yt_dlp.YoutubeDL(opts) as ydl:
        try:
            info = ydl.extract_info(video_url, download=False)

            subtitles = info.get('subtitles', {})
            auto_subs = info.get('automatic_captions', {})
            sub_dict = subtitles if subtitles else auto_subs

            if not sub_dict:
                return None, "no subtitles"

            for lang in ['en', 'en-US', 'en-GB', 'en-orig']:
                if lang in sub_dict:
                    for fmt in sub_dict[lang]:
                        if fmt.get('ext') == 'vtt':
                            try:
                                with urllib.request.urlopen(fmt['url'], timeout=30) as response:
                                    return response.read().decode('utf-8'), None
                            except urllib.error.HTTPError as e:
                                if e.code == 429:
                                    return None, "rate limited"
                                return None, f"HTTP {e.code}"
                            except Exception as e:
                                return None, str(e)[:20]

            return None, "no English subs"
        except Exception as e:
            error_msg = str(e)[:30]
            if "429" in error_msg:
                return None, "rate limited"
            return None, error_msg


def fetch_video_subtitles(video_url: str, ydl_opts: dict) -> tuple[Optional[str], Optional[str]]:
    """Fetch subtitles with retry on 429 rate-limit errors (3 attempts, exponential backoff)."""
    max_retries = 3
    for attempt in range(max_retries):
        content, error = _try_fetch_subtitles(video_url, ydl_opts)
        if error == "rate limited" and attempt < max_retries - 1:
            time.sleep(2 ** (attempt + 1))  # 2s, 4s
            continue
        return content, error
    return None, "rate limited"


async def process_channel(channel_input: str, output_path: Path, progress: ProgressLogger):
    """Process all videos from a channel."""
    channel_url = get_channel_url(channel_input)
    channel_name = get_channel_name(channel_input)

    # Check for existing video IDs (resume functionality)
    existing_ids = get_existing_video_ids(output_path)
    if existing_ids:
        print(f"Found existing file with {len(existing_ids)} videos. Will resume...")

    print(f"Fetching video list from {channel_url}...")

    ydl_opts = {
        'quiet': True,
        'no_warnings': True,
        'extract_flat': True,
        'ignoreerrors': True,
    }

    videos = []
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        try:
            # Fetch from both /videos and /shorts tabs
            for tab in ['/videos', '/shorts']:
                try:
                    result = ydl.extract_info(f"{channel_url}{tab}", download=False)
                    if result and 'entries' in result:
                        videos.extend([e for e in result['entries'] if e])
                except Exception:
                    pass  # Tab might not exist
        except Exception as e:
            print(f"Error fetching channel: {e}")
            return

    if not videos:
        print("No videos found.")
        return

    # Resume mode: filter out already-processed videos
    resuming = len(existing_ids) > 0
    if resuming:
        original_count = len(videos)
        videos = [v for v in videos if v.get('id') not in existing_ids]
        progress.resumed = original_count - len(videos)
        print(f"Found {original_count} videos, skipping {progress.resumed} already processed. Processing {len(videos)}...")
    else:
        print(f"Found {len(videos)} videos. Processing...")

    if not videos:
        print("All videos already processed.")
        return

    # Set total and start progress bar
    progress.total = len(videos)
    progress.start()

    # Only write header if this is a new file
    if not resuming:
        writer = MarkdownWriter(output_path, f"# {channel_name}\n\n")
    else:
        writer = MarkdownWriter(output_path, "", resume=True)

    rate_monitor = RateLimitMonitor(threshold=3)
    semaphore = asyncio.Semaphore(2)

    async def process_video(video_info: dict):
        async with semaphore:
            await asyncio.sleep(random.uniform(0.5, 1.5))
            if rate_monitor.should_abort:
                progress.advance("skipped", video_info.get('title', '?'))
                return None

            await rate_monitor.wait_if_paused()

            if rate_monitor.should_abort:
                progress.advance("skipped", video_info.get('title', '?'))
                return None

            video_id = video_info.get('id')
            title = video_info.get('title', 'Unknown Title')

            if not video_id:
                progress.advance("skipped", title)
                return None

            # Show current video being processed
            progress.set_current(title)

            video_url = f"https://www.youtube.com/watch?v={video_id}"

            loop = asyncio.get_event_loop()
            try:
                vtt_content, error_reason = await loop.run_in_executor(
                    None,
                    fetch_video_subtitles,
                    video_url,
                    {'quiet': True, 'no_warnings': True}
                )
            except Exception as e:
                progress.advance("failed", title)
                return None

            if not vtt_content:
                if error_reason == "rate limited":
                    await rate_monitor.record_rate_limit()
                    progress.advance("rate_limited", title)
                else:
                    await rate_monitor.record_success()
                    progress.advance("skipped", title)
                return None

            await rate_monitor.record_success()
            cleaned_text = clean_vtt_content(vtt_content)
            if not cleaned_text:
                progress.advance("skipped", title)
                return None

            section = format_markdown_section(video_id, title, cleaned_text)
            progress.advance("processed", title)
            return section

    tasks = [process_video(v) for v in videos]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    writer = MarkdownWriter(output_path, "", resume=True)
    for result in results:
        if isinstance(result, str) and result:
            writer.append(result)
    writer.close()


async def process_search(query: str, num_results: int, output_path: Path, progress: ProgressLogger):
    """Process top N videos from a YouTube search query."""

    # Check for existing video IDs (resume functionality)
    existing_ids = get_existing_video_ids(output_path)
    if existing_ids:
        print(f"Found existing file with {len(existing_ids)} videos. Will resume...")

    print(f"Searching YouTube for: {query} (top {num_results} results)...")

    ydl_opts = {
        'quiet': True,
        'no_warnings': True,
        'extract_flat': True,
        'ignoreerrors': True,
    }

    videos = []
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        try:
            result = ydl.extract_info(f"ytsearch{num_results}:{query}", download=False)
            if result and 'entries' in result:
                videos = [e for e in result['entries'] if e]
        except Exception as e:
            print(f"Error searching YouTube: {e}")
            return

    if not videos:
        print("No videos found.")
        return

    # Resume mode: filter out already-processed videos
    resuming = len(existing_ids) > 0
    if resuming:
        original_count = len(videos)
        videos = [v for v in videos if v.get('id') not in existing_ids]
        progress.resumed = original_count - len(videos)
        print(f"Found {original_count} videos, skipping {progress.resumed} already processed. Processing {len(videos)}...")
    else:
        print(f"Found {len(videos)} videos. Processing...")

    if not videos:
        print("All videos already processed.")
        return

    # Set total and start progress bar
    progress.total = len(videos)
    progress.start()

    # Only write header if this is a new file
    if not resuming:
        writer = MarkdownWriter(output_path, f"# Search: {query}\n\n")
    else:
        writer = MarkdownWriter(output_path, "", resume=True)

    rate_monitor = RateLimitMonitor(threshold=3)
    semaphore = asyncio.Semaphore(2)

    async def process_video(video_info: dict):
        async with semaphore:
            await asyncio.sleep(random.uniform(0.5, 1.5))
            if rate_monitor.should_abort:
                progress.advance("skipped", video_info.get('title', '?'))
                return None

            await rate_monitor.wait_if_paused()

            if rate_monitor.should_abort:
                progress.advance("skipped", video_info.get('title', '?'))
                return None

            video_id = video_info.get('id')
            title = video_info.get('title', 'Unknown Title')

            if not video_id:
                progress.advance("skipped", title)
                return None

            progress.set_current(title)

            video_url = f"https://www.youtube.com/watch?v={video_id}"

            loop = asyncio.get_event_loop()
            try:
                vtt_content, error_reason = await loop.run_in_executor(
                    None,
                    fetch_video_subtitles,
                    video_url,
                    {'quiet': True, 'no_warnings': True}
                )
            except Exception as e:
                progress.advance("failed", title)
                return None

            if not vtt_content:
                if error_reason == "rate limited":
                    await rate_monitor.record_rate_limit()
                    progress.advance("rate_limited", title)
                else:
                    await rate_monitor.record_success()
                    progress.advance("skipped", title)
                return None

            await rate_monitor.record_success()
            cleaned_text = clean_vtt_content(vtt_content)
            if not cleaned_text:
                progress.advance("skipped", title)
                return None

            section = format_markdown_section(video_id, title, cleaned_text)
            progress.advance("processed", title)
            return section

    tasks = [process_video(v) for v in videos]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    for result in results:
        if isinstance(result, str) and result:
            writer.append(result)
    writer.close()


def main():
    parser = argparse.ArgumentParser(
        description='Stream YouTube channel/search transcripts to Markdown',
        usage='yt-research @channel_name\n       yt-research "search query"\n       yt-research https://www.youtube.com/results?search_query=...'
    )
    parser.add_argument('input', help='Channel (@name or URL), search query, or YouTube search URL')
    parser.add_argument('-o', '--output', help='Output file path')
    parser.add_argument('-n', '--num-results', type=int, default=15, help='Number of search results to process (default: 15)')
    parser.add_argument('-v', '--verbose', action='store_true', help='Show detailed progress for each video')

    args = parser.parse_args()

    if is_search_input(args.input):
        query = get_search_query(args.input)
        if args.output:
            output_path = Path(args.output)
        else:
            output_path = Path.cwd() / f'{sanitize_filename(query)}.md'

        print(f"Output: {output_path}")

        progress = ProgressLogger(verbose=args.verbose, task_label="Processing videos",
                                   metrics=YT_PROGRESS_METRICS, show_eta=True)
        asyncio.run(process_search(query, args.num_results, output_path, progress))
    else:
        channel_name = get_channel_name(args.input)
        if args.output:
            output_path = Path(args.output)
        else:
            output_path = Path.cwd() / f'{channel_name}.md'

        print(f"Output: {output_path}")

        progress = ProgressLogger(verbose=args.verbose, task_label="Processing videos",
                                   metrics=YT_PROGRESS_METRICS, show_eta=True)
        asyncio.run(process_channel(args.input, output_path, progress))

    progress.summary()
    print(f"\nDone! Output written to: {output_path}")


if __name__ == '__main__':
    main()
