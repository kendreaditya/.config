#!/Users/kendreaditya/workspace/.venv/bin/python3
"""
shortn - LLM Context Compressor

Compresses large text/markdown files to fit within a specified token limit
using extractive summarization with per-section TextRank (no LLM required).
"""

import argparse
import os
import re
import sys
import time
from collections import defaultdict
from concurrent.futures import ProcessPoolExecutor, as_completed
from dataclasses import dataclass
from pathlib import Path
from typing import Optional

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from _context import split_sentences, get_tokenizer, count_tokens


def log(msg: str, verbose: bool = False, file=sys.stderr):
    """Print log message if verbose mode is enabled."""
    if verbose:
        print(f"[shortn] {msg}", file=file)


def get_words(text: str) -> set[str]:
    """Extract words from text for similarity calculation."""
    text = re.sub(r'[#*_`\[\]()]', ' ', text.lower())
    words = re.findall(r'\b[a-z]+\b', text)
    stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
                 'of', 'with', 'by', 'from', 'is', 'are', 'was', 'were', 'be', 'been',
                 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',
                 'could', 'should', 'may', 'might', 'must', 'shall', 'can', 'this',
                 'that', 'these', 'those', 'it', 'its', 'they', 'them', 'their', 'he',
                 'she', 'his', 'her', 'we', 'our', 'you', 'your', 'i', 'my', 'me'}
    return set(w for w in words if w not in stopwords and len(w) > 2)


def jaccard_similarity(set1: set, set2: set) -> float:
    """Calculate Jaccard similarity between two sets."""
    if not set1 or not set2:
        return 0.0
    intersection = len(set1 & set2)
    union = len(set1 | set2)
    return intersection / union if union > 0 else 0.0


def textrank_scores(sentences: list[str], damping: float = 0.85, iterations: int = 20) -> list[float]:
    """
    Compute TextRank scores for sentences.
    Pure Python implementation (no numpy required).
    """
    n = len(sentences)
    if n == 0:
        return []
    if n == 1:
        return [1.0]

    # Pre-compute word sets for each sentence
    word_sets = [get_words(s) for s in sentences]

    # Build similarity matrix (sparse representation)
    similarity = defaultdict(float)
    out_degree = [0.0] * n

    for i in range(n):
        for j in range(i + 1, n):
            sim = jaccard_similarity(word_sets[i], word_sets[j])
            if sim > 0:
                similarity[(i, j)] = sim
                similarity[(j, i)] = sim
                out_degree[i] += sim
                out_degree[j] += sim

    # Initialize scores
    scores = [1.0 / n] * n

    # Power iteration
    for _ in range(iterations):
        new_scores = [0.0] * n
        for i in range(n):
            rank_sum = 0.0
            for j in range(n):
                if i != j and out_degree[j] > 0:
                    sim = similarity.get((j, i), 0.0)
                    if sim > 0:
                        rank_sum += sim * scores[j] / out_degree[j]
            new_scores[i] = (1 - damping) / n + damping * rank_sum
        scores = new_scores

    return scores


@dataclass
class Section:
    """A document section with header and content units."""
    header: Optional[str]
    header_idx: int  # Original index in document
    units: list[str]  # Content units (sentences, lists, code blocks)
    unit_indices: list[int]  # Original indices in document


def classify_unit(text: str) -> str:
    """Classify a text unit type."""
    stripped = text.strip()
    if stripped.startswith('# ') and not stripped.startswith('## '):
        return 'title'
    if stripped.startswith('#'):
        return 'header'
    if stripped.startswith('```'):
        return 'code'
    if re.match(r'^[\s]*[-*+]|\d+\.', stripped):
        return 'list'
    return 'text'


def split_paragraph(text: str) -> list[str]:
    """Split a paragraph into sentences."""
    if not text.strip():
        return []

    # Check if it's a list (preserve as single unit)
    lines = text.strip().split('\n')
    if all(re.match(r'^[\s]*[-*+]|\d+\.', line) for line in lines if line.strip()):
        return [text]

    return split_sentences(text)


def parse_into_sections(text: str, verbose: bool = False) -> tuple[list[str], list[Section]]:
    """
    Parse document into sections, each starting with a header.
    Returns (all_units, sections) where all_units preserves original order.
    """
    log("Parsing document into sections...", verbose)
    start = time.time()

    lines = text.split('\n')
    all_units = []  # Flat list of all units with original indices
    sections = []

    current_section: Optional[Section] = None
    current_block = []
    in_code_block = False
    unit_idx = 0

    def flush_block():
        nonlocal unit_idx
        if current_block:
            para = '\n'.join(current_block)
            sentences = split_paragraph(para)
            for s in sentences:
                all_units.append(s)
                if current_section:
                    current_section.units.append(s)
                    current_section.unit_indices.append(unit_idx)
                unit_idx += 1
            current_block.clear()

    for line in lines:
        # Track code blocks
        if line.strip().startswith('```'):
            if in_code_block:
                current_block.append(line)
                code = '\n'.join(current_block)
                all_units.append(code)
                if current_section:
                    current_section.units.append(code)
                    current_section.unit_indices.append(unit_idx)
                unit_idx += 1
                current_block = []
                in_code_block = False
            else:
                flush_block()
                current_block.append(line)
                in_code_block = True
            continue

        if in_code_block:
            current_block.append(line)
            continue

        # Headers start new sections
        if line.strip().startswith('#'):
            flush_block()
            all_units.append(line)

            # Start new section
            current_section = Section(
                header=line,
                header_idx=unit_idx,
                units=[],
                unit_indices=[]
            )
            sections.append(current_section)
            unit_idx += 1
            continue

        # Empty lines separate paragraphs
        if not line.strip():
            flush_block()
            continue

        current_block.append(line)

    # Handle remaining content
    if current_block:
        if in_code_block:
            code = '\n'.join(current_block)
            all_units.append(code)
            if current_section:
                current_section.units.append(code)
                current_section.unit_indices.append(unit_idx)
        else:
            flush_block()

    log(f"Parsed {len(all_units):,} units into {len(sections):,} sections in {time.time() - start:.2f}s", verbose)
    return all_units, sections


def score_section(section_units: list[str]) -> list[float]:
    """Score units within a section using TextRank. Used for parallel processing."""
    if not section_units:
        return []
    return textrank_scores(section_units)


def compress_text(text: str, token_limit: int, tokenizer, verbose: bool = False, parallel: bool = True) -> str:
    """
    Compress text using per-section TextRank with parallel processing.
    """
    # Parse into sections
    all_units, sections = parse_into_sections(text, verbose)

    if not all_units:
        return ""

    # Handle headerless documents - create synthetic sections by chunking
    if not sections:
        log("No headers found - creating synthetic sections from chunks...", verbose)
        chunk_size = 500  # Units per synthetic section
        sections = []
        for i in range(0, len(all_units), chunk_size):
            chunk_units = all_units[i:i + chunk_size]
            chunk_indices = list(range(i, min(i + chunk_size, len(all_units))))
            sections.append(Section(
                header=None,
                header_idx=-1,
                units=chunk_units,
                unit_indices=chunk_indices
            ))
        log(f"Created {len(sections):,} synthetic sections", verbose)

    # Count tokens for each unit
    log("Counting tokens...", verbose)
    start = time.time()
    token_counts = [count_tokens(u, tokenizer) for u in all_units]
    total_tokens = sum(token_counts)
    log(f"Total tokens: {total_tokens:,}, Target: {token_limit:,} ({time.time() - start:.2f}s)", verbose)

    # If already under limit, return as-is
    if total_tokens <= token_limit:
        log("Already under token limit, returning original", verbose)
        return text

    # Classify all units
    unit_types = [classify_unit(u) for u in all_units]

    # Score sections in parallel
    log(f"Scoring {len(sections):,} sections using TextRank...", verbose)
    start = time.time()

    section_scores = {}  # section_idx -> list of scores for units in that section

    if parallel and len(sections) > 1:
        # Parallel scoring
        with ProcessPoolExecutor() as executor:
            futures = {
                executor.submit(score_section, sec.units): i
                for i, sec in enumerate(sections)
            }
            for future in as_completed(futures):
                sec_idx = futures[future]
                section_scores[sec_idx] = future.result()
    else:
        # Sequential scoring
        for i, sec in enumerate(sections):
            section_scores[i] = score_section(sec.units)

    log(f"Scoring completed in {time.time() - start:.2f}s", verbose)

    # Build global scores array
    scores = [0.0] * len(all_units)

    # Assign scores from section results
    for sec_idx, sec in enumerate(sections):
        sec_scores = section_scores.get(sec_idx, [])
        for i, unit_idx in enumerate(sec.unit_indices):
            if i < len(sec_scores):
                scores[unit_idx] = sec_scores[i]

    # Apply boosts
    for i, utype in enumerate(unit_types):
        if utype == 'title':
            scores[i] = float('inf')  # Always include title
        elif utype == 'header':
            scores[i] = 0  # Headers scored separately based on content
        elif utype == 'code':
            scores[i] *= 1.3  # Slight boost for code blocks

    # Calculate section token budgets (proportional to original size)
    section_tokens = []
    for sec in sections:
        header_tokens = token_counts[sec.header_idx] if sec.header and sec.header_idx >= 0 else 0
        content_tokens = sum(token_counts[idx] for idx in sec.unit_indices)
        section_tokens.append(header_tokens + content_tokens)

    total_section_tokens = sum(section_tokens)

    # Select content per section
    log("Selecting content per section...", verbose)
    selected_indices = set()
    remaining_budget = token_limit

    # First pass: always include title
    for i, utype in enumerate(unit_types):
        if utype == 'title':
            selected_indices.add(i)
            remaining_budget -= token_counts[i]

    # Allocate budget to sections proportionally
    for sec_idx, sec in enumerate(sections):
        if not sec.units:
            continue

        # Skip if header is title (already included)
        if sec.header and classify_unit(sec.header) == 'title':
            continue

        # Calculate this section's budget
        sec_proportion = section_tokens[sec_idx] / total_section_tokens if total_section_tokens > 0 else 0
        sec_budget = int(remaining_budget * sec_proportion)

        # Minimum budget to include at least something
        min_budget = min(100, sec_budget)
        sec_budget = max(min_budget, sec_budget)

        # Get scores for this section's units
        unit_score_pairs = []
        for i, unit_idx in enumerate(sec.unit_indices):
            unit_score_pairs.append((unit_idx, scores[unit_idx], token_counts[unit_idx]))

        # Sort by score descending
        unit_score_pairs.sort(key=lambda x: x[1], reverse=True)

        # Select best units within budget
        sec_selected = []
        sec_used = 0

        for unit_idx, score, tc in unit_score_pairs:
            if sec_used + tc <= sec_budget:
                sec_selected.append(unit_idx)
                sec_used += tc

        # Only include header if we selected some content and header exists
        if sec_selected and sec.header and sec.header_idx >= 0:
            selected_indices.add(sec.header_idx)

        selected_indices.update(sec_selected)

    log(f"Selected {len(selected_indices):,} units", verbose)

    # Build output preserving original order
    selected_list = sorted(selected_indices)

    result_parts = []
    for idx in selected_list:
        if result_parts:
            result_parts.append('\n\n')
        result_parts.append(all_units[idx])

    return ''.join(result_parts)


def main():
    parser = argparse.ArgumentParser(
        description='Compress text/markdown files to fit within a token limit using extractive summarization.',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  shortn input.md -t 8000                    # → input.compressed.md
  shortn input.md -t 32000 -o out.md         # Specify output file
  shortn input.md -t 8000 --stdout           # Print to stdout
  shortn input.md -t 8000 --no-parallel      # Disable parallel processing
  cat input.md | shortn -t 8000 --stdin --stdout  # Pipe mode
        """
    )

    parser.add_argument('input', nargs='?', help='Input file (use --stdin for pipe)')
    parser.add_argument('-t', '--tokens', type=int, required=True,
                        help='Target token limit')
    parser.add_argument('-o', '--output', help='Output file (default: input.compressed.md)')
    parser.add_argument('--model', default='cl100k_base',
                        help='Tiktoken model (default: cl100k_base)')
    parser.add_argument('--stdout', action='store_true',
                        help='Print output to stdout instead of file')
    parser.add_argument('--stdin', action='store_true',
                        help='Read input from stdin')
    parser.add_argument('-q', '--quiet', action='store_true',
                        help='Suppress progress messages')
    parser.add_argument('-v', '--verbose', action='store_true',
                        help='Show detailed progress messages')
    parser.add_argument('--no-parallel', action='store_true',
                        help='Disable parallel processing')

    args = parser.parse_args()

    verbose = args.verbose and not args.quiet

    # Validate input
    log("Starting shortn...", verbose)
    if args.stdin:
        if args.input:
            parser.error("Cannot specify both input file and --stdin")
        log("Reading from stdin...", verbose)
        text = sys.stdin.read()
        input_name = "stdin"
    else:
        if not args.input:
            parser.error("Input file required (or use --stdin)")
        input_path = Path(args.input)
        if not input_path.exists():
            print(f"Error: File not found: {args.input}", file=sys.stderr)
            sys.exit(1)
        log(f"Reading file: {args.input}", verbose)
        text = input_path.read_text()
        input_name = input_path.stem

    log(f"Input size: {len(text):,} characters", verbose)

    # Get tokenizer
    log("Loading tokenizer...", verbose)
    tokenizer = get_tokenizer(args.model)

    # Count original tokens
    log("Counting original tokens...", verbose)
    start = time.time()
    original_tokens = count_tokens(text, tokenizer)
    log(f"Original token count: {original_tokens:,} (took {time.time() - start:.2f}s)", verbose)

    # Compress
    log("Starting compression...", verbose)
    compress_start = time.time()
    compressed = compress_text(
        text,
        args.tokens,
        tokenizer,
        verbose=verbose,
        parallel=not args.no_parallel
    )
    log(f"Compression completed in {time.time() - compress_start:.2f}s", verbose)

    # Count compressed tokens
    compressed_tokens = count_tokens(compressed, tokenizer)

    # Output
    if args.stdout:
        print(compressed)
    else:
        if args.output:
            output_path = Path(args.output)
        elif args.stdin:
            output_path = Path('compressed.md')
        else:
            input_path = Path(args.input)
            output_path = input_path.parent / f"{input_path.stem}.compressed{input_path.suffix}"

        output_path.write_text(compressed)

        if not args.quiet:
            reduction = (1 - compressed_tokens / original_tokens) * 100 if original_tokens > 0 else 0
            print(f"Compressed: {original_tokens:,} tokens → {compressed_tokens:,} tokens ({reduction:.1f}% reduction)")
            print(f"Output: {output_path}")


if __name__ == '__main__':
    main()
